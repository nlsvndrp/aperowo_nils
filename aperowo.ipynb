{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0858a350",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efcbc9",
   "metadata": {},
   "source": [
    "### Define URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65c543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://amiv.ethz.ch/en/events/signup/6501bc6e5ff1d3cb04531966',\n",
    "    'https://rw.ethz.ch/cse-life/apero-2020.html',\n",
    "    # Add more URLs as needed.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28134d",
   "metadata": {},
   "source": [
    "### Scrape the pages and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e582c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl from: https://amiv.ethz.ch/en/events/signup/6501bc6e5ff1d3cb04531966\n",
      "Could not load visited URLs from visited_urls.json: Expecting value: line 1 column 1 (char 0)\n",
      "Found 'apero' in: https://amiv.ethz.ch/en/events/signup/6501bc6e5ff1d3cb04531966\n",
      "  Date: Not found\n",
      "  Time: Not found to Not found\n",
      "  Location: Not found\n"
     ]
    }
   ],
   "source": [
    "# Global list to store discovered \"apero\" data.\n",
    "found_apero = []\n",
    "\n",
    "def load_visited(filename):\n",
    "    \"\"\"Load visited URLs from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            visited_list = json.load(f)\n",
    "            return set(visited_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load visited URLs from {filename}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def save_visited(filename, visited):\n",
    "    \"\"\"Save visited URLs to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(visited), f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save visited URLs to {filename}: {e}\")\n",
    "\n",
    "def extract_event_details(soup):\n",
    "    \"\"\"\n",
    "    Attempts to extract event date and location from a BeautifulSoup\n",
    "    object. This is a heuristic approach that:\n",
    "      - Checks for a <time> element for the date.\n",
    "      - Checks for an element with CSS classes like 'location' or 'venue'.\n",
    "      - If those are not found, searches the plain text for patterns like \n",
    "        \"Location:\" or \"Venue:\".\n",
    "    Returns a tuple (date, location).\n",
    "    \"\"\"\n",
    "    date = \"Not found\"\n",
    "    start_time = \"Not found\"\n",
    "    end_time = \"Not found\"\n",
    "    location = \"Not found\"\n",
    "    full_text = soup.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    # Try to find a <time> element in the page\n",
    "    time_element = soup.find('time')\n",
    "    if time_element:\n",
    "        date = time_element.get_text(strip=True)\n",
    "    \n",
    "    # Try to find an element with class 'location' or 'venue'\n",
    "    location_element = soup.find(class_='location') or soup.find(class_='venue')\n",
    "    if location_element:\n",
    "        location = location_element.get_text(strip=True)\n",
    "    else:\n",
    "        # As a fallback, search for text patterns in the full page text.\n",
    "        loc_match = re.search(r'(?:Venue|Location)[:\\-]\\s*([A-Za-z0-9 ,.-]+)', full_text, re.IGNORECASE)\n",
    "        if loc_match:\n",
    "            location = loc_match.group(1).strip()\n",
    "            \n",
    "\n",
    "    ''' For the AMIV database, we can also check for specific patterns in the text:\n",
    "    - The date is usually in the format dd-./mm-./yyyy, hh:mm or d-./m-./yyyy, hh:mm.\n",
    "    - The location is often mentioned in the html-line after the date, usually preceded by a '/'. '''\n",
    "\n",
    "    #TODO: Fix this class-search!! (Fallback method in the next code-block)\n",
    "    event_div = soup.find(\"div\", \n",
    "                              class_=re.compile(r\"^jss\\d+$\"), \n",
    "                              string=re.compile(r'\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4}'))\n",
    "    if event_div:\n",
    "        event_text = event_div.get_text(\" \", strip=True)\n",
    "        # Search for a date and a time (start or time range) in the event block.\n",
    "        dt_match = re.search(\n",
    "            r'(\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4})(?:,\\s*|\\s+)'  # date part with comma or whitespace separator\n",
    "            r'(\\d{1,2}:\\d{2}(?::\\d{2})?)'                       # starting time\n",
    "            r'(?:\\s*-\\s*(\\d{1,2}:\\d{2}(?::\\d{2})?))?',          # optional end time (time range)\n",
    "            event_text\n",
    "        )\n",
    "        if dt_match:\n",
    "            date = dt_match.group(1).strip()\n",
    "            start_time = dt_match.group(2).strip()\n",
    "            if dt_match.group(3):\n",
    "                end_time = dt_match.group(3).strip()\n",
    "            else:\n",
    "                end_time = \"Not found\"\n",
    "        # For the location, assume the location is in the div immediately following event_div.\n",
    "        location_div = event_div.find_next_sibling(\"div\", class_=re.compile(r\"^jss\\d+$\"))\n",
    "        if location_div:\n",
    "            # Here we assume the location is the full text of that div.\n",
    "            location = location_div.get_text(strip=True)\n",
    "\n",
    "    # else:\n",
    "    #     # If not found, try to extract a weekday (English or German).\n",
    "    #     weekday_pattern = r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Montag|Dienstag|Mittwoch|Donnerstag|Freitag|Samstag|Sonntag)\\b'\n",
    "    #     weekday_match = re.search(weekday_pattern, full_text, re.IGNORECASE)\n",
    "    #     if weekday_match:\n",
    "    #         date = weekday_match.group(0).strip()\n",
    "\n",
    "    return date, start_time, end_time, location\n",
    "\n",
    "def crawl(url, domain, visited, depth=0, max_depth=3):\n",
    "    \"\"\"\n",
    "    Recursively crawl a URL (and its subpages) to find occurrences of 'apero'.\n",
    "    \n",
    "    When 'apero' is detected, extract a snippet, the page title, as well as\n",
    "    event details such as date and location, and store the data.\n",
    "    \n",
    "    Parameters:\n",
    "      - url: The page URL to crawl.\n",
    "      - domain: The domain to restrict the crawl (e.g., \"example.com\").\n",
    "      - visited: A set of URLs already crawled.\n",
    "      - depth: Current recursion level.\n",
    "      - max_depth: Maximum recursion depth allowed.\n",
    "    \"\"\"\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Skipping {url} due to status code {response.status_code}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else \"No title\"\n",
    "    \n",
    "    # Check if the word \"apero\" is present (case-insensitive)\n",
    "    if \"apero\" in html.lower():\n",
    "        # Extract event details (date and location)\n",
    "        event_date, event_start_time, event_end_time, location = extract_event_details(soup)\n",
    "        # Extract a snippet of up to 100 characters before and after the first occurrence of \"apero\" or \"aperitif\"\n",
    "        match = re.search(r'.{0,100}aper.{0,100}', html, re.IGNORECASE)\n",
    "        snippet = match.group(0) if match else \"Snippet not available\"\n",
    "        found_apero.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"snippet\": snippet,\n",
    "            \"date\": event_date,\n",
    "            \"start time\": event_start_time,\n",
    "            \"end time\": event_end_time,\n",
    "            \"location\": location\n",
    "        })\n",
    "        print(f\"Found 'apero' in: {url}\")\n",
    "        print(f\"  Date: {event_date}\")\n",
    "        print(f\"  Time: {event_start_time} to {event_end_time}\")\n",
    "        print(f\"  Location: {location}\")\n",
    "\n",
    "    # Find and process all <a> tags on the page\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "\n",
    "        # Resolve relative URLs to absolute URLs\n",
    "        absolute_url = urllib.parse.urljoin(url, href)\n",
    "        parsed_url = urllib.parse.urlparse(absolute_url)\n",
    "\n",
    "        # Only follow links within the same domain\n",
    "        if parsed_url.netloc != domain:\n",
    "            continue\n",
    "\n",
    "        # Skip common non-HTML file types (e.g., PDFs, images)\n",
    "        if any(absolute_url.lower().endswith(ext) for ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif']):\n",
    "            continue\n",
    "\n",
    "        # Recurse if not visited and within the depth limit.\n",
    "        if absolute_url not in visited and depth < max_depth:\n",
    "            time.sleep(1)  # Pause a bit to be polite to the server\n",
    "            crawl(absolute_url, domain, visited, depth + 1, max_depth)\n",
    "    \n",
    "# File to save visited URLs\n",
    "state_filename = \"visited_urls.json\"\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Starting crawl from: {url}\")\n",
    "    # Load visited state from previous runs (if exists)\n",
    "    visited = load_visited(state_filename)\n",
    "    domain = urllib.parse.urlparse(url).netloc\n",
    "    \n",
    "    # Begin crawling\n",
    "    crawl(url, domain, visited)\n",
    "    \n",
    "    # After crawling, save the updated visited state for future runs\n",
    "    save_visited(state_filename, visited)\n",
    "\n",
    "# Save the found \"apero\" data to a JSON file.\n",
    "output_filename = \"apero_results.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(found_apero, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Apero data saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Split the text into lines.\n",
    "    lines = full_text.splitlines()\n",
    "\n",
    "    # Iterate over lines to find a date pattern.\n",
    "    for i, line in enumerate(lines):\n",
    "        date_match = re.search(r'(\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4}),\\s*(\\d{1,2}:\\d{2}(?::\\d{2})?)', line)\n",
    "        if date_match:\n",
    "\n",
    "            print(f\"DEBUG: Matched line {i}: {line}\")\n",
    "            date = date_match.group(1).strip()\n",
    "            start_time = date_match.group(2).strip()\n",
    "\n",
    "            #The event end time is two html lines after the start time (for AMIV).\n",
    "            end_time = lines[i+2] if i + 2 < len(lines) else \"Not found\"\n",
    "    \n",
    "            # Check the next line for the location info\n",
    "            if i + 1 < len(lines):\n",
    "\n",
    "                # TODO: This does not work, because i+1 is not actually the next line but the next character!!!\n",
    "                print(f\"DEBUG: Location? {i+1}: {lines[i + 1]}\")\n",
    "                next_line = lines[i + 1]\n",
    "                # Using regex to capture the substring starting with the last slash.\n",
    "                # The pattern \".*(\\/\\S.*)$\" matches as much as possible, then captures the final slash and all following non-newline characters.\n",
    "                loc_match = re.search(r'.*(\\/\\s+.*)$', next_line)\n",
    "                if loc_match:\n",
    "                    location = loc_match.group(1).strip()\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
