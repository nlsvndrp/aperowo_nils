{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0858a350",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efcbc9",
   "metadata": {},
   "source": [
    "### Define URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65c543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apis = [\n",
    "    'https://api.amiv.ethz.ch/events/',\n",
    "    # Add more URLs as needed.\n",
    "]\n",
    "\n",
    "urls = [\n",
    "    'https://https://vseth.ethz.ch/events/',\n",
    "    # Add more URLs as needed.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28134d",
   "metadata": {},
   "source": [
    "### Scrape the pages and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37842eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://api.amiv.ethz.ch/events/?where=%7B%22%24or%22%3A+%5B%7B%22title_en%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22description_en%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22catchphrase_en%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22title_de%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22description_de%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22catchphrase_de%22%3A+%7B%22%24regex%22%3A+%22aper%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22title_en%22%3A+%7B%22%24regex%22%3A+%22food%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22description_en%22%3A+%7B%22%24regex%22%3A+%22food%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22catchphrase_en%22%3A+%7B%22%24regex%22%3A+%22food%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22title_de%22%3A+%7B%22%24regex%22%3A+%22essen%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22description_de%22%3A+%7B%22%24regex%22%3A+%22essen%22%2C+%22%24options%22%3A+%22i%22%7D%7D%2C+%7B%22catchphrase_de%22%3A+%7B%22%24regex%22%3A+%22essen%22%2C+%22%24options%22%3A+%22i%22%7D%7D%5D%7D\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=2\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=3\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=4\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=5\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=6\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=7\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=8\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=9\n",
      "Fetching: https://api.amiv.ethz.ch/events?where={\"$or\": [{\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}}, {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}}, {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}, {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}]}&page=10\n",
      "Found 235 events with 'apero' or 'food' in the title or description.\n",
      "Extracted information for 235 events and saved to apero_results.json.\n"
     ]
    }
   ],
   "source": [
    "'''If the website has an API, use it instead of scraping.'''\n",
    "\n",
    "# Normalize text by removing diacritical marks (accents).\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing diacritical marks (accents).\"\"\"\n",
    "    return ''.join(\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Filter dictionary for the API query (on the server side).\n",
    "def build_api_url(base_url, filter_dict):\n",
    "    \"\"\"Construct the full API URL with the 'where' query parameter.\"\"\"\n",
    "    query_string = urllib.parse.urlencode({\"where\": json.dumps(filter_dict)})\n",
    "    return f\"{base_url}?{query_string}\"\n",
    "\n",
    "# Filter function to check if an event contains \"apero\" in any of its text fields (locally).\n",
    "def event_contains_apero(event):\n",
    "    \"\"\"Check if any of the event's text fields contain 'apero'.\"\"\"\n",
    "    # Combine several relevant text fields.\n",
    "    fields = [\n",
    "        event.get(\"title_en\", \"\"),\n",
    "        event.get(\"description_en\", \"\"),\n",
    "        event.get(\"catchphrase_en\", \"\"),\n",
    "        event.get(\"title_de\", \"\"),\n",
    "        event.get(\"description_de\", \"\"),\n",
    "        event.get(\"catchphrase_de\", \"\")\n",
    "    ]\n",
    "    # Join the fields into one string, convert to lower case, and normalize.\n",
    "    combined_text = normalize_text(\" \".join(fields).lower())\n",
    "    # Return True if \"apero\" is found in the combined text.\n",
    "    return \"apero\" in combined_text\n",
    "\n",
    "# Fetch all events from the API, handling pagination.\n",
    "def fetch_all_events(base_url, filter_dict=None):\n",
    "    events = []\n",
    "    \n",
    "    # Build the initial URL with filter if provided.\n",
    "    url = build_api_url(base_url, filter_dict) if filter_dict else base_url\n",
    "    \n",
    "    while url:\n",
    "        print(\"Fetching:\", url)  # Debug: print the URL of the current page.\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Assuming the API returns a JSON object with a '_items' key for the list of events.\n",
    "        if isinstance(data, dict) and '_items' in data:\n",
    "            events.extend(data['_items'])\n",
    "            # Use the 'next' link if available.\n",
    "            url = data.get('_links', {}).get('next', {}).get('href')\n",
    "            # If the API returns a relative URL, join it with the base URL.\n",
    "            if url and not url.startswith('http'):\n",
    "\n",
    "                # Remove any trailing slashes from base_url before joining.\n",
    "                url = requests.compat.urljoin(base_url.rstrip('/'), url)\n",
    "\n",
    "        # If the API returns a list of events directly.\n",
    "        # This is less common but some APIs might do this.        \n",
    "        elif isinstance(data, list):\n",
    "            events.extend(data)\n",
    "            # Break out if it's just a list\n",
    "            url = None\n",
    "        else:\n",
    "            url = None\n",
    "    return events\n",
    "\n",
    "# Extract specific fields from an event.\n",
    "def extract_event_fields(event):\n",
    "    \"\"\"\n",
    "    Extract specific fields from an event:\n",
    "    - URL, extracted from the event's _links section\n",
    "    - Title (preferring title_en over title_de)\n",
    "    - Date (from time_start, in YYYY-MM-DD format)\n",
    "    - Start and end times (only hh:mm)\n",
    "    - Location\n",
    "    \"\"\"\n",
    "    # Extract the URL assuming it is found in the '_links' section.\n",
    "    url = event.get(\"_links\", {}).get(\"self\", {}).get(\"href\", \"\")\n",
    "    # Prefer the English title, fallback to the German title.\n",
    "    title = event.get(\"title_en\", event.get(\"title_de\", \"\"))\n",
    "    # Get the full start time string in ISO 8601 format.\n",
    "    time_start = event.get(\"time_start\", \"\")\n",
    "    date = time_start[:10] if time_start else \"\"\n",
    "    # Extract only hh:mm (characters at positions 11 to 15)\n",
    "    start_time = time_start[11:16] if time_start and len(time_start) >= 16 else \"\"\n",
    "    # Similarly extract end time in hh:mm from the provided time_end field.\n",
    "    time_end = event.get(\"time_end\", \"\")\n",
    "    end_time = time_end[11:16] if time_end and len(time_end) >= 16 else \"\"\n",
    "    # Get the location (if available)\n",
    "    location = event.get(\"location\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"location\": location\n",
    "    }\n",
    "\n",
    "# Fetch all events from the API and filter them for \"apero\".\n",
    "events_with_apero = fetch_all_events(apis[0], {\"$or\": [\n",
    "    {\"title_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"description_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"catchphrase_en\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"title_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"description_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"catchphrase_de\": {\"$regex\": \"aper\", \"$options\": \"i\"}},\n",
    "    {\"title_en\": {\"$regex\": \"food\", \"$options\": \"i\"}},\n",
    "    {\"description_en\": {\"$regex\": \"food\", \"$options\": \"i\"}},\n",
    "    {\"catchphrase_en\": {\"$regex\": \"food\", \"$options\": \"i\"}},\n",
    "    {\"title_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}},\n",
    "    {\"description_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}},\n",
    "    {\"catchphrase_de\": {\"$regex\": \"essen\", \"$options\": \"i\"}}\n",
    "]})\n",
    "print(f\"Found {len(events_with_apero)} events with 'apero' or 'food' in the title or description.\")\n",
    "\n",
    "# Assume events_with_apero is the list of events fetched via your API filtering.\n",
    "filtered_events = [extract_event_fields(event) for event in events_with_apero]\n",
    "\n",
    "# Write the filtered events to a JSON file.\n",
    "with open(\"apero_results.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(filtered_events, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Extracted information for {len(filtered_events)} events and saved to apero_results.json.\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e582c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl from: https://https://vseth.ethz.ch/events/\n",
      "Error fetching https://https://vseth.ethz.ch/events/: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //vseth.ethz.ch/events/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001B697E61F90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Apero data saved to apero_results.json\n"
     ]
    }
   ],
   "source": [
    "# Global list to store discovered \"apero\" data.\n",
    "found_apero = []\n",
    "\n",
    "def load_visited(filename):\n",
    "    \"\"\"Load visited URLs from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            visited_list = json.load(f)\n",
    "            return set(visited_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load visited URLs from {filename}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def save_visited(filename, visited):\n",
    "    \"\"\"Save visited URLs to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(visited), f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save visited URLs to {filename}: {e}\")\n",
    "\n",
    "def extract_event_details(soup):\n",
    "    \"\"\"\n",
    "    Attempts to extract event date and location from a BeautifulSoup\n",
    "    object. This is a heuristic approach that:\n",
    "      - Checks for a <time> element for the date.\n",
    "      - Checks for an element with CSS classes like 'location' or 'venue'.\n",
    "      - If those are not found, searches the plain text for patterns like \n",
    "        \"Location:\" or \"Venue:\".\n",
    "    Returns a tuple (date, location).\n",
    "    \"\"\"\n",
    "    date = \"Not found\"\n",
    "    start_time = \"Not found\"\n",
    "    end_time = \"Not found\"\n",
    "    location = \"Not found\"\n",
    "    full_text = soup.get_text(\"|\", strip=True)\n",
    "\n",
    "    # Try to find a <time> element in the page\n",
    "    time_element = soup.find('time')\n",
    "    if time_element:\n",
    "        date = time_element.get_text(strip=True)\n",
    "    \n",
    "    # Try to find an element with class 'location' or 'venue'\n",
    "    location_element = soup.find(class_='location') or soup.find(class_='venue')\n",
    "    if location_element:\n",
    "        location = location_element.get_text(strip=True)\n",
    "    else:\n",
    "        # As a fallback, search for text patterns in the full page text.\n",
    "        loc_match = re.search(r'(?:Venue|Location)[:\\-]\\s*([A-Za-z0-9 ,.-]+)', full_text, re.IGNORECASE)\n",
    "        if loc_match:\n",
    "            location = loc_match.group(1).strip()\n",
    "            \n",
    "\n",
    "    ''' For the AMIV database, we can also check for specific patterns in the text:\n",
    "    - The date is usually in the format dd-./mm-./yyyy, hh:mm or d-./m-./yyyy, hh:mm.\n",
    "    - The location is often mentioned in the html-line after the date, usually preceded by a '/'. '''\n",
    "\n",
    "    # #TODO: Fix this class-search!! (Fallback method in the next code-block)\n",
    "    # event_div = soup.find(\"div\", \n",
    "    #                           class_=re.compile(r\"^jss\\d+$\"), \n",
    "    #                           string=re.compile(r'\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4}'))\n",
    "    # if event_div:\n",
    "    #     event_text = event_div.get_text(\" \", strip=True)\n",
    "    #     # Search for a date and a time (start or time range) in the event block.\n",
    "    #     dt_match = re.search(\n",
    "    #         r'(\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4})(?:,\\s*|\\s+)'  # date part with comma or whitespace separator\n",
    "    #         r'(\\d{1,2}:\\d{2}(?::\\d{2})?)'                       # starting time\n",
    "    #         r'(?:\\s*-\\s*(\\d{1,2}:\\d{2}(?::\\d{2})?))?',          # optional end time (time range)\n",
    "    #         event_text\n",
    "    #     )\n",
    "    #     if dt_match:\n",
    "    #         date = dt_match.group(1).strip()\n",
    "    #         start_time = dt_match.group(2).strip()\n",
    "    #         if dt_match.group(3):\n",
    "    #             end_time = dt_match.group(3).strip()\n",
    "    #         else:\n",
    "    #             end_time = \"Not found\"\n",
    "    #     # For the location, assume the location is in the div immediately following event_div.\n",
    "    #     location_div = event_div.find_next_sibling(\"div\", class_=re.compile(r\"^jss\\d+$\"))\n",
    "    #     if location_div:\n",
    "    #         # Here we assume the location is the full text of that div.\n",
    "    #         location = location_div.get_text(strip=True)\n",
    "\n",
    "        # Split the text into lines.\n",
    "    lines = full_text.splitlines()\n",
    "\n",
    "    # Iterate over lines to find a date pattern.\n",
    "    for i, line in enumerate(lines):\n",
    "        date_match = re.search(r'(\\d{1,2}[-\\.\\/]\\d{1,2}[-\\.\\/]\\d{4}),\\s*(\\d{1,2}:\\d{2}(?::\\d{2})?)', line)\n",
    "        if date_match:\n",
    "\n",
    "            print(f\"DEBUG: Matched line {i}: {line}\")\n",
    "            date = date_match.group(1).strip()\n",
    "            start_time = date_match.group(2).strip()\n",
    "\n",
    "            #The event end time is two html lines after the start time (for AMIV).\n",
    "            end_time = lines[i+2] if i + 2 < len(lines) else \"Not found\"\n",
    "    \n",
    "            # Check the next line for the location info\n",
    "            if i + 1 < len(lines):\n",
    "\n",
    "                # TODO: This does not work, because i+1 is not actually the next line but the next character!!!\n",
    "                print(f\"DEBUG: Location? {i+1}: {lines[i + 1]}\")\n",
    "                next_line = lines[i + 1]\n",
    "                # Using regex to capture the substring starting with the last slash.\n",
    "                # The pattern \".*(\\/\\S.*)$\" matches as much as possible, then captures the final slash and all following non-newline characters.\n",
    "                loc_match = re.search(r'.*(\\/\\s+.*)$', next_line)\n",
    "                if loc_match:\n",
    "                    location = loc_match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # else:\n",
    "    #     # If not found, try to extract a weekday (English or German).\n",
    "    #     weekday_pattern = r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Montag|Dienstag|Mittwoch|Donnerstag|Freitag|Samstag|Sonntag)\\b'\n",
    "    #     weekday_match = re.search(weekday_pattern, full_text, re.IGNORECASE)\n",
    "    #     if weekday_match:\n",
    "    #         date = weekday_match.group(0).strip()\n",
    "\n",
    "    return date, start_time, end_time, location\n",
    "\n",
    "def crawl(url, domain, visited, depth=0, max_depth=3):\n",
    "    \"\"\"\n",
    "    Recursively crawl a URL (and its subpages) to find occurrences of 'apero'.\n",
    "    \n",
    "    When 'apero' is detected, extract a snippet, the page title, as well as\n",
    "    event details such as date and location, and store the data.\n",
    "    \n",
    "    Parameters:\n",
    "      - url: The page URL to crawl.\n",
    "      - domain: The domain to restrict the crawl (e.g., \"example.com\").\n",
    "      - visited: A set of URLs already crawled.\n",
    "      - depth: Current recursion level.\n",
    "      - max_depth: Maximum recursion depth allowed.\n",
    "    \"\"\"\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Skipping {url} due to status code {response.status_code}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    html = response.text\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else \"No title\"\n",
    "    \n",
    "    # Check if the word \"apero\" is present (case-insensitive)\n",
    "    if \"apero\" in html.lower():\n",
    "        # Extract event details (date and location)\n",
    "        event_date, event_start_time, event_end_time, location = extract_event_details(soup)\n",
    "        # Extract a snippet of up to 100 characters before and after the first occurrence of \"apero\" or \"aperitif\"\n",
    "        match = re.search(r'.{0,100}apero.{0,100}', html, re.IGNORECASE)\n",
    "        snippet = match.group(0) if match else \"Snippet not available\"\n",
    "        found_apero.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"snippet\": snippet,\n",
    "            \"date\": event_date,\n",
    "            \"start time\": event_start_time,\n",
    "            \"end time\": event_end_time,\n",
    "            \"location\": location\n",
    "        })\n",
    "        print(f\"Found 'apero' in: {url}\")\n",
    "        print(f\"  Date: {event_date}\")\n",
    "        print(f\"  Time: {event_start_time} to {event_end_time}\")\n",
    "        print(f\"  Location: {location}\")\n",
    "\n",
    "    # Find and process all <a> tags on the page\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "\n",
    "        # Resolve relative URLs to absolute URLs\n",
    "        absolute_url = urllib.parse.urljoin(url, href)\n",
    "        parsed_url = urllib.parse.urlparse(absolute_url)\n",
    "\n",
    "        # Only follow links within the same domain\n",
    "        if parsed_url.netloc != domain:\n",
    "            continue\n",
    "\n",
    "        # Skip common non-HTML file types (e.g., PDFs, images)\n",
    "        if any(absolute_url.lower().endswith(ext) for ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif']):\n",
    "            continue\n",
    "\n",
    "        # Recurse if not visited and within the depth limit.\n",
    "        if absolute_url not in visited and depth < max_depth:\n",
    "            time.sleep(1)  # Pause a bit to be polite to the server\n",
    "            crawl(absolute_url, domain, visited, depth + 1, max_depth)\n",
    "    \n",
    "# File to save visited URLs\n",
    "state_filename = \"visited_urls.json\"\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Starting crawl from: {url}\")\n",
    "    # Load visited state from previous runs (if exists)\n",
    "    visited = load_visited(state_filename)\n",
    "    domain = urllib.parse.urlparse(url).netloc\n",
    "    \n",
    "    # Begin crawling\n",
    "    crawl(url, domain, visited)\n",
    "    \n",
    "    # After crawling, save the updated visited state for future runs\n",
    "    save_visited(state_filename, visited)\n",
    "\n",
    "# Save the found \"apero\" data to a JSON file.\n",
    "output_filename = \"apero_results.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(found_apero, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Apero data saved to {output_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
